{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["#imports\n","\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torchvision.utils import make_grid"],"metadata":{"id":"IlOUo0nKjYoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMx3R1wHfuuH"},"outputs":[],"source":["#model\n","\n","class LambdaLayer(nn.Module):\n","    def __init__(self, lambd):\n","        super(LambdaLayer, self).__init__()\n","        self.lambd = lambd\n","\n","    def forward(self, x):\n","        return self.lambd(x)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        #self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.gn1 = nn.GroupNorm(2,out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        #self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.gn2 = nn.GroupNorm(2,out_channels)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.shortcut = LambdaLayer(lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, out_channels//4, out_channels//4), \"constant\", 0))\n","            \n","\n","    def forward(self, x):\n","        out = F.relu(self.gn1(self.conv1(x)))\n","        out = self.gn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","def _weights_init(m):\n","    # classname = m.__class__.__name__\n","    # print(classname)\n","    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        init.kaiming_normal_(m.weight)\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=100):\n","        super(ResNet, self).__init__()\n","        self.in_channels = 16\n","\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n","        #self.bn1 = nn.BatchNorm2d(16)\n","        self.gn1 = nn.GroupNorm(2,16)\n","        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n","        self.linear = nn.Linear(64, num_classes)\n","\n","        self.apply(_weights_init)\n","\n","    def _make_layer(self, block, out_channels, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_channels, out_channels, stride))\n","            self.in_channels = out_channels * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.gn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = F.avg_pool2d(out, out.size()[3])\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet20():\n","    model = ResNet(BasicBlock, [3, 3, 3])\n","    print('ResNet20')\n","\n","    total_params = 0\n","    for x in filter(lambda p: p.requires_grad, model.parameters()):\n","        total_params += np.prod(x.data.numpy().shape)\n","    print(\"Total number of params\", total_params)\n","    print(\"Total layers\", len(list(filter(\n","        lambda p: p.requires_grad and len(p.data.size()) > 1, model.parameters()))))\n","\n","    return model"]},{"cell_type":"code","source":["model = ResNet20()"],"metadata":{"id":"NLgkpT4UiTmJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#hyperparams\n","\n","batch_size = 128\n","learning_rate = 0.1\n","momentum = 0.9\n","weight_decay = 0.0001\n","num_epochs = 160"],"metadata":{"id":"8WRAUHtxmqJE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#data\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomCrop(32, 4),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n","                          std=[0.2675, 0.2565, 0.2761])\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n","                          std=[0.2675, 0.2565, 0.2761])\n","])\n","\n","train_dataset = torchvision.datasets.CIFAR100(root='./data',download=True, transform=train_transform)\n","test_dataset = torchvision.datasets.CIFAR100(root='./data',download=True, train=False, transform=test_transform)\n","\n","train_dl = DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True)\n","test_dl = DataLoader(test_dataset, batch_size, pin_memory=True)\n","\n","def show_batch(dl):\n","  for images,labels in dl:\n","    fig,ax=plt.subplots(figsize=(10,10))\n","    ax.imshow(make_grid(images,10).permute(1,2,0))\n","    break"],"metadata":{"id":"MUQ5D0Xkjssm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_batch(train_dl)"],"metadata":{"id":"pOiN4Nndk6Pd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#device\n","\n","def get_default_device():\n","  return torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","\n","def to_device(entity, device):\n","  if isinstance(entity,(list,tuple)):\n","    return [to_device(elem, device) for elem in entity]\n","  return entity.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","  #wrapper around dataloaders to transfer batches to devices\n","  def __init__ (self, dataloader, device):\n","    self.dl = dataloader\n","    self.device = device\n","\n","  def __iter__(self):\n","    for b in self.dl:\n","      yield to_device(b, self.device)\n","\n","  def __len__(self):\n","    return len(self.dl)"],"metadata":{"id":"fMOoxikNnBg7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = get_default_device()\n","train_dl = DeviceDataLoader(train_dl,device)\n","test_dl = DeviceDataLoader(test_dl,device)"],"metadata":{"id":"M7bjDSMlnqdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#training\n","\n","def accuracy(logits,labels):\n","  pred, predClassId = torch.max(logits,dim=1)\n","  return torch.tensor(torch.sum(predClassId == labels).item() / len(logits))\n","\n","def evaluate(model,dl,loss_func):\n","  model.eval()\n","  batch_losses, batch_accs = [], []\n","  for images,labels in dl:\n","    with torch.no_grad():\n","      logits = model(images)\n","    batch_losses.append(loss_func(logits,labels))\n","    batch_accs.append(accuracy(logits,labels))\n","    epoch_avg_loss = torch.stack(batch_losses).mean()\n","    epoch_avg_acc = torch.stack(batch_accs).mean()\n","    return epoch_avg_loss, epoch_avg_acc\n","\n","def train(model, train_dl, num_epochs, loss_func, optimizer):\n","  results = []\n","  for epoch in range(num_epochs):\n","    model.train()\n","    for images, labels in train_dl:\n","      logits = model(images)\n","      loss = loss_func(logits,labels)\n","      loss.backward()\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","  epoch_avg_loss,epoch_avg_acc = evaluate(model,train_dl,loss_func)  \n","  results.append({'avg_loss':epoch_avg_loss,'avg_acc':epoch_avg_acc})\n","\n","  return results"],"metadata":{"id":"tmdRv12jqBCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.cuda()\n","loss_func = nn.CrossEntropyLoss().cuda()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n","                            momentum=momentum,\n","                            weight_decay=weight_decay)\n","\n","results = train(model,train_dl,num_epochs,loss_func,optimizer)"],"metadata":{"id":"9yhTcOTvn6Gn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results"],"metadata":{"id":"fUp4t0C-8aeZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save\n","if not os.path.exists('./model'):\n","  os.makedirs('./model')\n","  \n","torch.save(model.state_dict(), './model/resnet20groupNorm.pth')"],"metadata":{"id":"rxyE-ooV8b5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#test\n","model1 = ResNet20().cuda()\n","model1.load_state_dict(torch.load('./model/resnet20groupNorm.pth'))\n","_, test_acc = evaluate(model1,test_dl,loss_func)\n","print(test_acc)"],"metadata":{"id":"KlbA72Eo8jgR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671965255586,"user_tz":0,"elapsed":17,"user":{"displayName":"user","userId":"02220475389831152211"}},"outputId":"d10ca061-ca92-4e78-f2e9-c0dae640ce25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet20\n","Total number of params 275572\n","Total layers 20\n","tensor(0.6016)\n"]}]}]}